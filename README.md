# Neural-Network-Based-House-Price-Prediction-A-Custom-Implementation
ğŸ¯ Quick Overview

This project implements a custom neural network from scratch (no TensorFlow, no PyTorch, no pretrained models) to predict house prices based on various features. The implementation focuses on preventing overfitting through advanced regularization techniques and scientific experimentation.
Key Highlights
âœ… 100% Custom Implementation - Built using only NumPy

âœ… No Pretrained Models - Everything coded from scratch
âœ… Anti-Overfitting Focus - Early stopping, dropout, L2 regularization
âœ… Manifold Learning - PCA and t-SNE for data exploration
âœ… Scientific Approach - 6 different architectures tested
âœ… Excellent Generalization - Train-Val gap < 0.05
ğŸŒ SDG Alignment
SDG 11: Sustainable Cities and Communities
This project contributes to making cities inclusive, safe, resilient, and sustainable through:

ğŸ˜ï¸ Affordable Housing: Accurate price predictions help identify fairly-priced properties
ğŸ“Š Data-Driven Planning: ML insights assist urban development decisions
ğŸ’¡ Market Transparency: Provides objective price assessments
ğŸ¤ Accessibility: Helps homebuyers make informed decisions


âœ¨ Features
Data Processing

Missing value imputation with intelligent strategies
Outlier detection and handling (IQR method)
Feature engineering with interaction terms
RobustScaler for better generalization

Manifold Learning

PCA Analysis - Dimensionality reduction and variance analysis
t-SNE Visualization - Non-linear pattern discovery
Feature Importance - Identify key price drivers

Neural Network

Custom activation functions (ReLU, Leaky ReLU, Tanh)
Multiple optimizers (SGD, Momentum, Adam)
Dropout regularization (0.3-0.5)
L2 regularization
Early stopping mechanism
Learning rate decay
Gradient clipping

Anti-Overfitting Techniques

âœ… Early stopping with patience
âœ… Dropout layers
âœ… L2 weight regularization
âœ… Smaller architectures
âœ… Gradient clipping
âœ… Learning rate decay


ğŸ“ Project Structure


week1_data_preprocessing.ipynb
week2_neural_network.ipynb
week3_hyperparameter_tuning.ipynb

Individual Components
Week 1: Data Preprocessing
python# Includes:
# - Dataset creation with missing values
# - EDA and visualizations
# - Manifold learning (PCA, t-SNE)
# - Feature engineering
# - Data scaling
Week 2: Neural Network Training
python# Includes:
# - Custom NN implementation
# - Forward/backward propagation
# - Early stopping
# - Model evaluation
# - Anti-overfitting techniques
Week 3: Hyperparameter Tuning
python# Includes:
# - 6 different architectures
# - Optimizer comparison
# - Regularization experiments
# - Best model selection
# - Comprehensive analysis

ğŸ”¬ Technical Details
Neural Network Architecture
Best Model Configuration:
python{
    'Architecture': [n_features, 32, 16, 1],
    'Activation': 'ReLU',
    'Optimizer': 'Adam',
    'Learning Rate': 0.01 (with decay),
    'L2 Regularization': 0.01,
    'Dropout': 0.3,
    'Batch Size': 64,
    'Early Stopping': Patience 150
}
Activation Functions
ReLU (Primary)
f(x) = max(0, x)
f'(x) = 1 if x > 0, else 0
Leaky ReLU
f(x) = x if x > 0, else 0.01x
f'(x) = 1 if x > 0, else 0.01
Loss Function
Mean Squared Error with L2 Regularization
L = (1/m) Î£(y_pred - y_true)Â² + (Î»/2m) Î£(wÂ²)
Optimization
Adam Optimizer
m = Î²â‚m + (1-Î²â‚)âˆ‡L
v = Î²â‚‚v + (1-Î²â‚‚)(âˆ‡L)Â²
w = w - Î± * mÌ‚ / (âˆšvÌ‚ + Îµ)

ğŸ“Š Results
Performance Metrics
MetricTrainingValidationTestRÂ² Score0.8830.8710.868RMSE (scaled)0.3420.3590.363RMSE (original)$28,450$29,820$30,150MAE (original)$21,300$22,100$22,450
Key Achievements

âœ… Train-Val Gap: 0.017 (Excellent generalization!)
âœ… Early Stopping: Activated at epoch 847/1500
âœ… No Overfitting: Curves stay together
âœ… Manifold Analysis: 6 components explain 95% variance

Comparative Analysis
ModelRÂ²RMSE ($)GapStatusBalanced (Best)0.868$30,1500.017ğŸ† WinnerWide Shallow0.863$31,2000.023âœ… GoodConservative0.851$32,8000.012âœ… SafeModerate Depth0.859$31,5000.041âš ï¸ Slight overfitMomentum0.854$32,1000.028âŒ Adam betterAggressive Reg0.842$33,9000.009âŒ Too restrictive
